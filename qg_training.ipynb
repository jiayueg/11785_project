{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/AMontgomerie/question_generator/blob/master/training/qg_training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FvBm_K5WnVj9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UDaysJyJytAs"
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset race (C:\\Users\\Kevin\\.cache\\huggingface\\datasets\\race\\high\\0.1.0\\5a80ba2d003e023fdce95d01c1b02f5a70d5eb2375465bee162baf9824c91474)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de09448ad7146179c64bc4f26b5f4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"race\", \"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PzW_zmk2qFHG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-IgF44jMFPY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRETRAINED_MODEL = 't5-base'\n",
    "DIR = \"question_generator/\"\n",
    "BATCH_SIZE = 1\n",
    "SEQ_LENGTH = 512\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "tokenizer.add_special_tokens(\n",
    "    {'additional_special_tokens': ['<answer>', '<context>']}\n",
    ")\n",
    "\n",
    "# class QGDataset(Dataset):\n",
    "#     def __init__(self, csv):\n",
    "#         self.df = pd.read_csv(csv, engine='python')\n",
    "\n",
    "#     def __len__(self):\n",
    "#          return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):   \n",
    "#         if torch.is_tensor(idx):\n",
    "#             idx = idx.tolist()\n",
    "#         row = self.df.iloc[idx, 1:]       \n",
    "\n",
    "#         encoded_text = tokenizer(\n",
    "#             row['text'], \n",
    "#             pad_to_max_length=True, \n",
    "#             max_length=SEQ_LENGTH,\n",
    "#             truncation=True,\n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "#         encoded_text['input_ids'] = torch.squeeze(encoded_text['input_ids'])\n",
    "#         encoded_text['attention_mask'] = torch.squeeze(encoded_text['attention_mask'])\n",
    "\n",
    "#         encoded_question = tokenizer(\n",
    "#             row['question'],\n",
    "#             pad_to_max_length=True,\n",
    "#             max_length=SEQ_LENGTH,\n",
    "#             truncation=True,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "#         encoded_question['input_ids'] = torch.squeeze(encoded_question['input_ids'])\n",
    "\n",
    "#         return (encoded_text.to(device), encoded_question.to(device))\n",
    "\n",
    "# train_set = QGDataset(os.path.join(DIR, 'question_generator/datasets/qg_train.csv'))\n",
    "# train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# valid_set = QGDataset(os.path.join(DIR, 'question_generator/datasets/qg_valid.csv')) \n",
    "# valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b7ffc9bf904eceb1e4c99186e1bf12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3498 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\anaconda3\\envs\\idl_p\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8734f35747246aa97f8932c27d7bdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62445 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_text(row):    \n",
    "    encoded = {}\n",
    "    encoded_text = tokenizer(\n",
    "        row['answer'] + row['article'], \n",
    "        pad_to_max_length=True, \n",
    "        max_length=SEQ_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded['input_ids'] = torch.squeeze(encoded_text['input_ids'])\n",
    "    encoded['attention_mask'] = torch.squeeze(encoded_text['attention_mask'])\n",
    "\n",
    "    encoded_question = tokenizer(\n",
    "        row['question'],\n",
    "        pad_to_max_length=True,\n",
    "        max_length=SEQ_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    encoded['input_ids_question'] = torch.squeeze(encoded_question['input_ids'])\n",
    "    return encoded\n",
    "\n",
    "dataset = dataset.map(make_text)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'input_ids_question'])\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset[\"validation\"], batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NJrb9kYNz_wz"
   },
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "EPOCHS = 20\n",
    "LOG_INTERVAL = 5000\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "config = T5Config(decoder_start_token_id=tokenizer.pad_token_id)\n",
    "model = T5ForConditionalGeneration(config).from_pretrained(PRETRAINED_MODEL)\n",
    "model.resize_token_embeddings(len(tokenizer)) # to account for new special tokens\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rcfRh2JC0CF1"
   },
   "outputs": [],
   "source": [
    "SAVED_MODEL_PATH = \"question_generator/qg_pretrained_t5_model_trained.pth\"\n",
    "\n",
    "def train(epoch, best_val_loss):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        target = {\n",
    "            'input_ids': batch['input_ids_question'].to(device)\n",
    "        }\n",
    "        data = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        optimizer.zero_grad()\n",
    "        masked_labels = mask_label_padding(target['input_ids'])\n",
    "        output = model(\n",
    "            input_ids=data['input_ids'],\n",
    "            attention_mask=data['attention_mask'],\n",
    "            labels=masked_labels\n",
    "        )\n",
    "        loss = output[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch_index % LOG_INTERVAL == 0 and batch_index > 0:\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            print('| epoch {:3d} | ' \n",
    "                  '{:5d}/{:5d} batches | '\n",
    "                  'loss {:5.2f}'.format(\n",
    "                    epoch, \n",
    "                    batch_index, len(train_loader), \n",
    "                    cur_loss))\n",
    "            save(\n",
    "                TEMP_SAVE_PATH,\n",
    "                epoch, \n",
    "                model.state_dict(), \n",
    "                optimizer.state_dict(), \n",
    "                best_val_loss\n",
    "            )\n",
    "            total_loss = 0\n",
    "\n",
    "def evaluate(eval_model, data_loader):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch in enumerate(data_loader):\n",
    "            target = {\n",
    "                'input_ids': batch['input_ids_question'].to(device)\n",
    "            }\n",
    "            data = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)\n",
    "            }\n",
    "            masked_labels = mask_label_padding(target['input_ids'])\n",
    "            output = eval_model(\n",
    "                input_ids=data['input_ids'],\n",
    "                attention_mask=data['attention_mask'],\n",
    "                labels=masked_labels\n",
    "            )\n",
    "            total_loss += output[0].item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def mask_label_padding(labels):\n",
    "    MASK_ID = -100\n",
    "    labels[labels==tokenizer.pad_token_id] = MASK_ID\n",
    "    return labels\n",
    "\n",
    "def save(path, epoch, model_state_dict, optimizer_state_dict, loss):\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_state_dict,\n",
    "            'optimizer_state_dict': optimizer_state_dict,\n",
    "            'best_loss': loss,\n",
    "            }, path)\n",
    "\n",
    "def load(path):\n",
    "    return torch.load(path)\n",
    "\n",
    "def print_line():\n",
    "    LINE_WIDTH = 60\n",
    "    print('-' * LINE_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9J13rDps2QIu"
   },
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "# val_loss = evaluate(model, valid_loader)\n",
    "# print_line()\n",
    "# print('| Before training | valid loss {:5.2f}'.format(\n",
    "#     val_loss)\n",
    "# )\n",
    "# print_line()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    train(epoch, best_val_loss)\n",
    "    val_loss = evaluate(model, valid_loader)\n",
    "    print_line()\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f}'.format(\n",
    "        epoch,\n",
    "        val_loss)\n",
    "    )\n",
    "    print_line()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "        save(\n",
    "             SAVED_MODEL_PATH,\n",
    "             epoch, \n",
    "             model.state_dict(), \n",
    "             optimizer.state_dict(), \n",
    "             best_val_loss\n",
    "        )\n",
    "        print(\"| Model saved.\")\n",
    "        print_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNkFJbYAaLQ9Q2ylMwrLSAk",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1Dv4wvUk6kl0WGFBxf2zb2W_3DDzwBLe8",
   "name": "QG_T5_training.ipynb",
   "provenance": [
    {
     "file_id": "1fjUac5wqsbl4kdfR6XRD0Zor3TfAg-fe",
     "timestamp": 1592456064028
    },
    {
     "file_id": "1y_8MTWmQnKalcdTQNLSZaoWN2KuAmUC5",
     "timestamp": 1591767629922
    },
    {
     "file_id": "1CIlJj2br71COiwuF4ZLWJPnMNgu0Z3u5",
     "timestamp": 1590466302923
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python [conda env:idl_p]",
   "language": "python",
   "name": "conda-env-idl_p-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
