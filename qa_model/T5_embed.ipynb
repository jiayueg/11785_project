{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4f984c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "177f19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "     AdamW,\n",
    "     T5ForConditionalGeneration,\n",
    "     T5Tokenizer,\n",
    "     get_linear_schedule_with_warmup,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e650a827",
   "metadata": {},
   "source": [
    "# Load, Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9516e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"./TOEFL-QA\")\n",
    "from utils import load_data, load_data_workhorse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5a1cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, dev_data, test_data = load_data(\"TOEFL-QA/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "557d05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_form(data_dict):\n",
    "    new_dict = {}\n",
    "    new_dict[\"sentences_word\"] = [] #each word is treated spereately and sentence is viewed as a list\n",
    "    new_dict[\"sentences_join\"] = [] #words in sentence are joined, each sentence is viewed as a string\n",
    "    new_dict[\"questions_word\"] = []\n",
    "    new_dict[\"questions_join\"] = []\n",
    "    new_dict[\"answers_word\"] = []\n",
    "    new_dict[\"answers_join\"] = []\n",
    "    new_dict[\"options_word\"] = [] #should be a list of list, word-level\n",
    "    new_dict[\"options_join\"] = []\n",
    "    new_dict[\"context\"] = []\n",
    "    for file in data_dict.keys():\n",
    "        #can also have word-based version\n",
    "        new_dict[\"sentences_word\"].append(data_dict[file][\"sentences\"])\n",
    "        sentences_list = [(lambda words_list: ' '.join(words_list)) (words_list) for words_list in data_dict[file][\"sentences\"]]\n",
    "        new_dict[\"sentences_join\"].append(sentences_list)\n",
    "        new_dict[\"context\"] = [(lambda sentence_list: ' '.join(sentence_list)) (sentence_list) for sentence_list in new_dict[\"sentences_join\"]]\n",
    "        \n",
    "        new_dict['questions_join'].append(\" \".join(data_dict[file][\"question\"]))\n",
    "        new_dict['questions_word'].append(data_dict[file][\"question\"])\n",
    "        \n",
    "        new_dict['answers_join'].append(\" \".join(data_dict[file][\"answer\"]))\n",
    "        new_dict['answers_word'].append(data_dict[file][\"answer\"])\n",
    "        \n",
    "        new_dict[\"options_join\"].append([(lambda words_list: ' '.join(words_list)) (words_list) for words_list in data_dict[file][\"options\"]])\n",
    "        new_dict[\"options_word\"].append(data_dict[file][\"options\"])\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8667a2f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_transform = convert_form(train_data)\n",
    "train = datasets.Dataset.from_dict(train_transform) #here the dataset refers to the transformer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cff049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_transform = convert_form(dev_data)\n",
    "dev = datasets.Dataset.from_dict(dev_transform) #here the dataset refers to the transformer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60cacb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = convert_form(test_data)\n",
    "test = datasets.Dataset.from_dict(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ef5b362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'why does the student go to the career services office'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][\"questions_join\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35060188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to find out he is allowed to attend the career fair'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][\"answers_join\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a659fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi , do you have a minute ? sure , how can i help you ? i have a couple of questions about the career fair next week . um well , are seniors the only ones who can go ? i mean , you know , they are finishing school this year and getting their degrees and everything . and , well , it seems like businesses would wanna talk to them and not first year students like me . no , no , the career fair is opened to all our students and we encourage anyone who's interested to go check it out . well , that's good to know . you've seen the flyers and posters around campus , i assume . sure , can't miss them . i mean , they all say where and when the fair is , just not who should attend . actually they do , but it's in the small print . uh , we should probably make that part easier to reach , shouldn't we ? i'll make a note of that right now . so , do you have any other questions ? yes , actually i do now . um since i'd only be going to familiarize myself with the process , you know , check it out , i was wondering if there is anything youwould recommend that i do to prepare . that's actually a very good question . well , as you know , the career fair is generally an opportunity for local businesses to recruit new employees , and for soon to be graduates to have interviews with several companies they might be interested in working for . now , in your case , even though you wouldn't be looking for employment right now , it still wouldn't hurt for you to prepare much like you would if you were looking for a job . you mean , like get my resume together and wear a suit ? that's a given . i was thinking more along the lines of doing some research . the flyers and posters list all the businesses that are sending representatives to the career fair . um what's your major or do you to have one yet ? well , i haven't declared a major yet , but i'm strongly considering accounting . see , that's part of the reason i wanna go to the fair , to help me decide if that's what i really want to study . that's very wise . well , i suggest that you get on the computer and learn more about the accounting companies in particular that would be attending . you can learn a lot about companies from their internet websites . then prepare a list of questions . questions , hmm so , in a way , i'll be interviewing them ? that's one way of looking at it . think about it for a second . what do you want to know about working for an accounting firm ? well , there is the job itself , and salary of course , and working conditions , i mean , would i have an office , or would i work in a big room with a zillion other employees , and and maybe about opportunities for advancement . see ? those're all important things to know . after you do some research , you'll be able to tailor your questions to the particular company you are talking to . wow , i'm glad i came by here . so , it looks like i've got some work to do . and if you plan on attending future career fairs , i recommend you sign up for one of our interview workshops .\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10e519c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi , do you have a minute ?',\n",
       " 'sure , how can i help you ?',\n",
       " 'i have a couple of questions about the career fair next week .',\n",
       " 'um well , are seniors the only ones who can go ?',\n",
       " 'i mean , you know , they are finishing school this year and getting their degrees and everything .',\n",
       " 'and , well , it seems like businesses would wanna talk to them and not first year students like me .',\n",
       " \"no , no , the career fair is opened to all our students and we encourage anyone who's interested to go check it out .\",\n",
       " \"well , that's good to know .\",\n",
       " \"you've seen the flyers and posters around campus , i assume .\",\n",
       " \"sure , can't miss them .\",\n",
       " 'i mean , they all say where and when the fair is , just not who should attend .',\n",
       " \"actually they do , but it's in the small print .\",\n",
       " \"uh , we should probably make that part easier to reach , shouldn't we ?\",\n",
       " \"i'll make a note of that right now .\",\n",
       " 'so , do you have any other questions ?',\n",
       " 'yes , actually i do now .',\n",
       " \"um since i'd only be going to familiarize myself with the process , you know , check it out , i was wondering if there is anything youwould recommend that i do to prepare .\",\n",
       " \"that's actually a very good question .\",\n",
       " 'well , as you know , the career fair is generally an opportunity for local businesses to recruit new employees , and for soon to be graduates to have interviews with several companies they might be interested in working for .',\n",
       " \"now , in your case , even though you wouldn't be looking for employment right now , it still wouldn't hurt for you to prepare much like you would if you were looking for a job .\",\n",
       " 'you mean , like get my resume together and wear a suit ?',\n",
       " \"that's a given .\",\n",
       " 'i was thinking more along the lines of doing some research .',\n",
       " 'the flyers and posters list all the businesses that are sending representatives to the career fair .',\n",
       " \"um what's your major or do you to have one yet ?\",\n",
       " \"well , i haven't declared a major yet , but i'm strongly considering accounting .\",\n",
       " \"see , that's part of the reason i wanna go to the fair , to help me decide if that's what i really want to study .\",\n",
       " \"that's very wise .\",\n",
       " 'well , i suggest that you get on the computer and learn more about the accounting companies in particular that would be attending .',\n",
       " 'you can learn a lot about companies from their internet websites .',\n",
       " 'then prepare a list of questions .',\n",
       " \"questions , hmm so , in a way , i'll be interviewing them ?\",\n",
       " \"that's one way of looking at it .\",\n",
       " 'think about it for a second .',\n",
       " 'what do you want to know about working for an accounting firm ?',\n",
       " 'well , there is the job itself , and salary of course , and working conditions , i mean , would i have an office , or would i work in a big room with a zillion other employees , and and maybe about opportunities for advancement .',\n",
       " 'see ?',\n",
       " \"those're all important things to know .\",\n",
       " \"after you do some research , you'll be able to tailor your questions to the particular company you are talking to .\",\n",
       " \"wow , i'm glad i came by here .\",\n",
       " \"so , it looks like i've got some work to do .\",\n",
       " 'and if you plan on attending future career fairs , i recommend you sign up for one of our interview workshops .']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][\"sentences_join\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2a163",
   "metadata": {},
   "source": [
    "# tokenize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3867472",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='t5-small', vocab_size=32100, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccd5b15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'why does the student go to the career services office'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][\"questions_join\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17aa998b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (841 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "encode = tokenizer(train[0][\"questions_join\"], train[0]['context'])['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2f78348",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca66bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_trainset = tokenizer(\n",
    "    train[\"questions_join\"],\n",
    "    train[\"context\"],\n",
    "    truncation=\"only_second\",\n",
    "    max_length=384,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=128,\n",
    "    padding=\"max_length\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c3026bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mapping = tokenized_trainset.pop(\"overflow_to_sample_mapping\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "614d3b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 33,\n",
       " 33,\n",
       " 33,\n",
       " 33,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 35,\n",
       " 35,\n",
       " 35,\n",
       " 35,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 37,\n",
       " 37,\n",
       " 37,\n",
       " 37,\n",
       " 37,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 41,\n",
       " 41,\n",
       " 41,\n",
       " 41,\n",
       " 41,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 43,\n",
       " 43,\n",
       " 43,\n",
       " 43,\n",
       " 43,\n",
       " 44,\n",
       " 44,\n",
       " 44,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 47,\n",
       " 47,\n",
       " 47,\n",
       " 47,\n",
       " 47,\n",
       " 48,\n",
       " 48,\n",
       " 48,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 51,\n",
       " 51,\n",
       " 51,\n",
       " 51,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 54,\n",
       " 54,\n",
       " 54,\n",
       " 54,\n",
       " 55,\n",
       " 55,\n",
       " 55,\n",
       " 55,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 57,\n",
       " 57,\n",
       " 57,\n",
       " 58,\n",
       " 58,\n",
       " 58,\n",
       " 58,\n",
       " 59,\n",
       " 59,\n",
       " 59,\n",
       " 59,\n",
       " 59,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 63,\n",
       " 63,\n",
       " 63,\n",
       " 63,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 65,\n",
       " 65,\n",
       " 65,\n",
       " 65,\n",
       " 65,\n",
       " 66,\n",
       " 66,\n",
       " 66,\n",
       " 66,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 68,\n",
       " 68,\n",
       " 68,\n",
       " 69,\n",
       " 69,\n",
       " 69,\n",
       " 69,\n",
       " 69,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 71,\n",
       " 71,\n",
       " 71,\n",
       " 72,\n",
       " 72,\n",
       " 73,\n",
       " 73,\n",
       " 73,\n",
       " 73,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 75,\n",
       " 75,\n",
       " 75,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 77,\n",
       " 77,\n",
       " 77,\n",
       " 77,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 79,\n",
       " 79,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 82,\n",
       " 82,\n",
       " 82,\n",
       " 82,\n",
       " 82,\n",
       " 83,\n",
       " 83,\n",
       " 83,\n",
       " 83,\n",
       " 83,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 88,\n",
       " 88,\n",
       " 88,\n",
       " 88,\n",
       " 88,\n",
       " 89,\n",
       " 89,\n",
       " 89,\n",
       " 89,\n",
       " 89,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 91,\n",
       " 91,\n",
       " 91,\n",
       " 91,\n",
       " 91,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 92,\n",
       " 93,\n",
       " 93,\n",
       " 93,\n",
       " 93,\n",
       " 93,\n",
       " 94,\n",
       " 94,\n",
       " 94,\n",
       " 94,\n",
       " 95,\n",
       " 95,\n",
       " 95,\n",
       " 96,\n",
       " 96,\n",
       " 96,\n",
       " 96,\n",
       " 96,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 98,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 110,\n",
       " 111,\n",
       " 111,\n",
       " 111,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 112,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 113,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 115,\n",
       " 116,\n",
       " 116,\n",
       " 116,\n",
       " 117,\n",
       " 117,\n",
       " 117,\n",
       " 118,\n",
       " 118,\n",
       " 118,\n",
       " 118,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 121,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 122,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 123,\n",
       " 124,\n",
       " 124,\n",
       " 124,\n",
       " 124,\n",
       " 124,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 125,\n",
       " 126,\n",
       " 126,\n",
       " 126,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 129,\n",
       " 129,\n",
       " 129,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 131,\n",
       " 131,\n",
       " 131,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 134,\n",
       " 134,\n",
       " 134,\n",
       " 134,\n",
       " 134,\n",
       " 135,\n",
       " 135,\n",
       " 135,\n",
       " 135,\n",
       " 135,\n",
       " 136,\n",
       " 136,\n",
       " 136,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 137,\n",
       " 138,\n",
       " 138,\n",
       " 138,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 141,\n",
       " 141,\n",
       " 141,\n",
       " 141,\n",
       " 141,\n",
       " 142,\n",
       " 142,\n",
       " 142,\n",
       " 142,\n",
       " 142,\n",
       " 143,\n",
       " 143,\n",
       " 143,\n",
       " 144,\n",
       " 144,\n",
       " 144,\n",
       " 145,\n",
       " 145,\n",
       " 145,\n",
       " 145,\n",
       " 145,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 147,\n",
       " 147,\n",
       " 147,\n",
       " 147,\n",
       " 147,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 148,\n",
       " 149,\n",
       " 149,\n",
       " 149,\n",
       " 149,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 151,\n",
       " 152,\n",
       " 152,\n",
       " 152,\n",
       " 152,\n",
       " 152,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 153,\n",
       " 154,\n",
       " 154,\n",
       " 154,\n",
       " 155,\n",
       " 155,\n",
       " 155,\n",
       " 156,\n",
       " 156,\n",
       " 156,\n",
       " 156,\n",
       " 157,\n",
       " 157,\n",
       " 157,\n",
       " 157,\n",
       " 157,\n",
       " 158,\n",
       " 158,\n",
       " 158,\n",
       " 158,\n",
       " 158,\n",
       " 159,\n",
       " 159,\n",
       " 159,\n",
       " 159,\n",
       " 159,\n",
       " 160,\n",
       " 160,\n",
       " 160,\n",
       " 161,\n",
       " 161,\n",
       " 161,\n",
       " 161,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 163,\n",
       " 163,\n",
       " 163,\n",
       " 163,\n",
       " 163,\n",
       " 164,\n",
       " 164,\n",
       " 164,\n",
       " 164,\n",
       " 164,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 166,\n",
       " 166,\n",
       " 166,\n",
       " 167,\n",
       " 167,\n",
       " 167,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 168,\n",
       " 169,\n",
       " 169,\n",
       " 169,\n",
       " 169,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 170,\n",
       " 171,\n",
       " 171,\n",
       " 171,\n",
       " 171,\n",
       " 171,\n",
       " 172,\n",
       " 172,\n",
       " 172,\n",
       " 172,\n",
       " 173,\n",
       " 173,\n",
       " 173,\n",
       " 174,\n",
       " 174,\n",
       " 174,\n",
       " 174,\n",
       " 174,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 176,\n",
       " 176,\n",
       " 176,\n",
       " 176,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 178,\n",
       " 178,\n",
       " 178,\n",
       " 179,\n",
       " 179,\n",
       " 179,\n",
       " 179,\n",
       " 179,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 180,\n",
       " 181,\n",
       " 181,\n",
       " 181,\n",
       " 182,\n",
       " 182,\n",
       " 182,\n",
       " 182,\n",
       " 183,\n",
       " 183,\n",
       " 183,\n",
       " 183,\n",
       " 183,\n",
       " 184,\n",
       " 184,\n",
       " 184,\n",
       " 184,\n",
       " 185,\n",
       " 185,\n",
       " 186,\n",
       " 186,\n",
       " 186,\n",
       " 187,\n",
       " 187,\n",
       " 187,\n",
       " 187,\n",
       " 187,\n",
       " 188,\n",
       " 188,\n",
       " 188,\n",
       " 188,\n",
       " 188,\n",
       " 189,\n",
       " 189,\n",
       " 189,\n",
       " 190,\n",
       " 190,\n",
       " 190,\n",
       " 191,\n",
       " 191,\n",
       " 191,\n",
       " 191,\n",
       " 191,\n",
       " 192,\n",
       " 192,\n",
       " 192,\n",
       " 192,\n",
       " 192,\n",
       " 193,\n",
       " 193,\n",
       " 193,\n",
       " 193,\n",
       " 193,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 195,\n",
       " 195,\n",
       " 195,\n",
       " 195,\n",
       " 196,\n",
       " 196,\n",
       " 196,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 198,\n",
       " 198,\n",
       " 198,\n",
       " 199,\n",
       " 199,\n",
       " 199,\n",
       " 199,\n",
       " 199,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 201,\n",
       " 201,\n",
       " 201,\n",
       " 202,\n",
       " 202,\n",
       " 202,\n",
       " 202,\n",
       " 203,\n",
       " 203,\n",
       " 203,\n",
       " 203,\n",
       " 204,\n",
       " 204,\n",
       " 204,\n",
       " 204,\n",
       " 205,\n",
       " 205,\n",
       " 205,\n",
       " 206,\n",
       " 206,\n",
       " 206,\n",
       " 206,\n",
       " 206,\n",
       " 207,\n",
       " 207,\n",
       " 207,\n",
       " 207,\n",
       " 207,\n",
       " 208,\n",
       " 208,\n",
       " 208,\n",
       " 208,\n",
       " 209,\n",
       " 209,\n",
       " 209,\n",
       " 209,\n",
       " 210,\n",
       " 210,\n",
       " 210,\n",
       " 210,\n",
       " 211,\n",
       " 211,\n",
       " 211,\n",
       " 211,\n",
       " 212,\n",
       " 212,\n",
       " 212,\n",
       " 213,\n",
       " 213,\n",
       " 213,\n",
       " 213,\n",
       " 213,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 215,\n",
       " 215,\n",
       " 215,\n",
       " 215,\n",
       " 215,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 216,\n",
       " 217,\n",
       " 217,\n",
       " 217,\n",
       " 217,\n",
       " 217,\n",
       " 218,\n",
       " 218,\n",
       " 218,\n",
       " 218,\n",
       " 218,\n",
       " 219,\n",
       " 219,\n",
       " 219,\n",
       " 219,\n",
       " 219,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 220,\n",
       " 221,\n",
       " 221,\n",
       " 221,\n",
       " 222,\n",
       " 222,\n",
       " 222,\n",
       " 222,\n",
       " 222,\n",
       " 223,\n",
       " 223,\n",
       " 223,\n",
       " 224,\n",
       " 224,\n",
       " 224,\n",
       " 225,\n",
       " 225,\n",
       " 225,\n",
       " 225,\n",
       " 225,\n",
       " 226,\n",
       " 226,\n",
       " 226,\n",
       " 226,\n",
       " 226,\n",
       " 227,\n",
       " 227,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 229,\n",
       " 229,\n",
       " 229,\n",
       " 229,\n",
       " 229,\n",
       " 230,\n",
       " 230,\n",
       " 230,\n",
       " 231,\n",
       " 231,\n",
       " 231,\n",
       " 232,\n",
       " 232,\n",
       " 232,\n",
       " 232,\n",
       " 233,\n",
       " 233,\n",
       " 233,\n",
       " 234,\n",
       " 234,\n",
       " 234,\n",
       " 234,\n",
       " 234,\n",
       " 235,\n",
       " 235,\n",
       " 235,\n",
       " 235,\n",
       " 235,\n",
       " 236,\n",
       " 236,\n",
       " 236,\n",
       " 237,\n",
       " 237,\n",
       " 237,\n",
       " 237,\n",
       " 238,\n",
       " 238,\n",
       " 238,\n",
       " 238,\n",
       " 239,\n",
       " 239,\n",
       " 239,\n",
       " 239,\n",
       " 239,\n",
       " 239,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 241,\n",
       " 241,\n",
       " 241,\n",
       " 241,\n",
       " 241,\n",
       " 242,\n",
       " 242,\n",
       " 242,\n",
       " 242,\n",
       " 242,\n",
       " 243,\n",
       " 243,\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_mapping #map from indices of training instance to actual index question to account for truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33ad9a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"why does the student go to the career services office</s>, i haven't declared a major yet, but i'm strongly considering accounting. see, that's part of the reason i wanna go to the fair, to help me decide if that's what i really want to study. that's very wise. well, i suggest that you get on the computer and learn more about the accounting companies in particular that would be attending. you can learn a lot about companies from their internet websites. then prepare a list of questions. questions, hmm so, in a way, i'll be interviewing them? that's one way of looking at it. think about it for a second. what do you want to know about working for an accounting firm? well, there is the job itself, and salary of course, and working conditions, i mean, would i have an office, or would i work in a big room with a zillion other employees, and and maybe about opportunities for advancement. see? those're all important things to know. after you do some research, you'll be able to tailor your questions to the particular company you are talking to. wow, i'm glad i came by here. so, it looks like i've got some work to do. and if you plan on attending future career fairs, i recommend you sign up for one of our interview workshops.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_trainset[\"input_ids\"][2]) #example question and context pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11df8d6",
   "metadata": {},
   "source": [
    "# Tokenize And Encode Answer As Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d79a50ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_encoding = tokenizer(\n",
    "     train['answers_join'],\n",
    "     max_length=24,\n",
    "     padding='max_length',\n",
    "     truncation=True,\n",
    "     return_attention_mask=True,\n",
    " )\n",
    " \n",
    "labels = answer_encoding[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03e8929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define pytorch dataset for TOEFL-QA\n",
    "class BioQADataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe,\n",
    "        tokenizer:T5Tokenizer,\n",
    "        source_max_token_len: int = 396,\n",
    "        source_stride: int = 128,\n",
    "        target_max_token_len: int = 32,\n",
    "    ):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_max_token_len =  source_max_token_len\n",
    "        self.source_stride = source_stride\n",
    "        self.target_max_token_len =  target_max_token_len\n",
    "        \n",
    "        #tokenize input\n",
    "        self.source_encoding = tokenizer(\n",
    "            self.dataframe[\"questions_join\"],\n",
    "            self.dataframe[\"context\"],\n",
    "            truncation=\"only_second\",\n",
    "            max_length=self.source_max_token_len,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_attention_mask=True,\n",
    "            stride=self.source_stride,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        #tokenize output\n",
    "        self.target_encoding = tokenizer(\n",
    "            self.dataframe[\"answers_join\"],\n",
    "            truncation=True,\n",
    "            max_length=self.target_max_token_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        #this is the mapping from indices of samples to indices of question\n",
    "        self.sample2indices = self.source_encoding.pop(\"overflow_to_sample_mapping\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.source_encoding))\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        label_idx = self.sample2indices[idx] # index of corresponding label\n",
    "        labels = self.target_encoding['input_ids'][label_idx]\n",
    "        labels[labels==0] = -100\n",
    "        \n",
    "        return dict(\n",
    "            question=self.dataframe[idx][\"questions_join\"],\n",
    "            context=self.dataframe[idx]['context'],\n",
    "            answer=self.dataframe[idx]['answers_join'],\n",
    "            input_ids=self.source_encoding[\"input_ids\"][idx].flatten(),\n",
    "            attention_mask=self.source_encoding['attention_mask'][idx].flatten(),\n",
    "            labels=labels.flatten(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44bcf19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data,\n",
    "        dev_data,\n",
    "        test_data,\n",
    "        tokenizer:T5Tokenizer,\n",
    "        batch_size: int = 8,\n",
    "        source_max_token_len: int = 396,\n",
    "        source_stride: int = 128,\n",
    "        target_max_token_len: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.dev_data = dev_data\n",
    "        self.test_data = test_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.source_stride = source_stride\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "        \n",
    "    def setup(self):\n",
    "        self.train_dataset = BioQADataset(\n",
    "            self.train_data,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.source_stride,\n",
    "            self.target_max_token_len,\n",
    "        )\n",
    "        self.dev_dataset = BioQADataset(\n",
    "            self.dev_data,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.source_stride,\n",
    "            self.target_max_token_len,\n",
    "        )\n",
    "        self.test_dataset = BioQADataset(\n",
    "            self.test_data,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.source_stride,\n",
    "            self.target_max_token_len,\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "        )\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dev_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=4,\n",
    "        )\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=1,\n",
    "            num_workers=4,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53b9eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "N_EPOCHS = 6\n",
    "data_module = BioDataModule(train, dev, test, tokenizer, batch_size=BATCH_SIZE)\n",
    "data_module.setup() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b93ccd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model definition\n",
    "class QAModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\"t5-small\", return_dict=True)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.model(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        return output.loss, output.logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask=batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\":outputs, \"labels\": labels}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask=batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask=batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=0.0001)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d1ed7",
   "metadata": {},
   "source": [
    "# genrating answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2349b585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/jiayueg/miniconda3/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 185.79 MiB already allocated; 4.56 MiB free; 196.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-dd4a126b8dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m      \u001b[0mprogress_bar_refresh_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m  )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_configure_sharded_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# allow user to setup in model sharded environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# note: this sets up self.lightning_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, trainer, model)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \"\"\"\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_nvidia_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, trainer, model)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mLightningModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_training_type_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_optimizers_in_pre_dispatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36msetup_training_type_plugin\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup_training_type_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.LightningModule\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;34m\"\"\"Attaches the training type plugin to the accelerator.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup_precision_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/single_device.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/single_device.py\u001b[0m in \u001b[0;36mmodel_to_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmodel_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DeviceDtypeModuleMixin\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    670\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 185.79 MiB already allocated; 4.56 MiB free; 196.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "model = QAModel()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "     max_epochs=20,\n",
    "     gpus=1,\n",
    "     progress_bar_refresh_rate = 30\n",
    " )\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46490a",
   "metadata": {},
   "source": [
    "# question answering example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d5546cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_encoding=tokenizer(\n",
    "       test[0][\"questions_join\"],\n",
    "       test[0]['context'],\n",
    "       max_length = 396,\n",
    "       padding=\"max_length\",\n",
    "       truncation=\"only_second\",\n",
    "       return_attention_mask=True,\n",
    "       add_special_tokens=True,\n",
    "       return_tensors=\"pt\"\n",
    "   )\n",
    "generated_ids = model.model.generate(\n",
    "       input_ids=source_encoding[\"input_ids\"],\n",
    "       attention_mask=source_encoding[\"attention_mask\"],\n",
    "       num_beams=1,  # greedy search\n",
    "       max_length=24,\n",
    "       repetition_penalty=2.5,\n",
    "       early_stopping=True,\n",
    "       use_cache=True\t\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5284c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True) for generated_id in generated_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c50eecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart k was born in austria hungary.\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ddb24fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "according to the professor, why was bartk music popular in austria hungary\n",
      "so i just finished reviewing your papers on the influence of nationalism on the composers music . and initially i was surprised none of you chose to write about b la bart k , that is until i remembered we haven't had a chance to discuss him in class yet . he was a wonderful and ground breaking composer . b la bart k was a hungarian , whose life stretched from the late nineteenth century to the middle of twentieth century . but he was not a fan of the romantic style of music that was popular in his homeland during his youth . wait , hungary wasn't a country in 19 , was it ? you are right . i should have been clear . bartok was born in austria hungary , a nation that broke apart when he was about forty years old . actually , the town where he was born is presently part of romania . the political history of that region is complex . suffice to say that bartok is generally known as a hungarian composer . so during bart k's youth , the music played in the concert halls of austria hungary was dominated by romantic pieces by mostly german composers . we discussed the romantic style last week . these pieces were long and lyrical . they were meant to have a sort of grandeur about them . and in the early 19 's , composers who worked in the romantic style were the most popular in austria hungary . but bartok , he was part of the musical community that was trying to change this . and it led him to well , the first thing it did was lead him to travel . he looked at the countryside for the music of the farmers and the people who lived in small towns , and their music , well , you could say he discovered the music that was popular in those areas . well , all the music we have been talking about the past few weeks , it really was all in the cities , that's where the composers and the orchestras were . out in remote areas of the countryside , in rural locations , music was more traditional , the same songs that were enjoyed by previous generations . bart k went out , he travelled to a significant portion of eastern europe actually . he roamed the countryside and listened to the music heard in small towns and in all sorts of celebrations . he attended weddings , dances and religious ceremonies , where he heard a very different sort of music from the romantic stuff being played in the concert halls in the cities . the music he heard is what we would consider folk music . and then he had those same songs played in the concert halls ? no . at first he went around to document the folk music . he really wanted to make sure the folk songs were written down before they disappeared . in fact , bart k didn't start out the trip thinking of himself as a composer . he was an ethnomusicologist . he studied the traditional music of the region . but it turns out that what would later have a notable influence on european music on the whole , was the way bart k used elements he heard in folk songs in his own compositions . he adopted a number of elements from what he heard , like unusual rhythms . and he liked to use the glissando as his hallmark , which he probably got from listening to croatian folk music . a glissando is well , i have got a recording of bartok here . let's wait until the music is fresh in our minds susie , do you have something you want to ask first ? yeah . before , you mention nationalism and ah , right , yes . when bartok had his new pieces performed , their folk music roots made them instantly popular . it happened to be a time of strong nationalism in austria hungary , so his compositions came at just the right time . he became very successful there . particularly , when bartok's ballet the wooden prince opened , there was great excitement for music that included musical elements from local folk songs , music that reflected the region's musical traditions . however , as popular as bartok was in his homeland , he did not get much international recognition during his lifetime .\n",
      "bartk compositions incorporated music from the local culture\n"
     ]
    }
   ],
   "source": [
    "print(test[0][\"questions_join\"])\n",
    "print(test[0][\"context\"])\n",
    "print(test[0][\"answers_join\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38463954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
