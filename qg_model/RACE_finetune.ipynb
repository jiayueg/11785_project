{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source & about the dataset\n",
    "\n",
    "Code for this file is based on https://github.com/AMontgomerie/question_generator. The code is under an MIT license\n",
    "\n",
    "Race source: https://www.cs.cmu.edu/~glai1/data/race/ \n",
    "\n",
    "Race is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. The dataset can be served as the training and test sets for machine comprehension.\n",
    "\n",
    "The dataset itself is collected from https://huggingface.co/datasets/race"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FvBm_K5WnVj9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UDaysJyJytAs"
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset race (C:\\Users\\Kevin\\.cache\\huggingface\\datasets\\race\\high\\0.1.0\\5a80ba2d003e023fdce95d01c1b02f5a70d5eb2375465bee162baf9824c91474)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01652b91bd048ce92bf38dc70d15c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"race\", \"high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PzW_zmk2qFHG",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-IgF44jMFPY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRETRAINED_MODEL = 't5-base'\n",
    "DIR = \"question_generator/\"\n",
    "BATCH_SIZE = 1\n",
    "SEQ_LENGTH = 512\n",
    "EPOCHS = 3\n",
    "USE_ANSWER = False\n",
    "BEST = \"race_finetune_withanswer_epoch3.pt\"\n",
    "BEST_HF = \"race_finetune_withanswer_epoch3\"\n",
    "\n",
    "# Check whether the specified path exists or not\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "tokenizer.add_special_tokens(\n",
    "    {'additional_special_tokens': ['<answer>', '<context>']}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Kevin\\.cache\\huggingface\\datasets\\race\\high\\0.1.0\\5a80ba2d003e023fdce95d01c1b02f5a70d5eb2375465bee162baf9824c91474\\cache-dde5553f59d1442b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Kevin\\.cache\\huggingface\\datasets\\race\\high\\0.1.0\\5a80ba2d003e023fdce95d01c1b02f5a70d5eb2375465bee162baf9824c91474\\cache-aee9e17dc574fd1a.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Kevin\\.cache\\huggingface\\datasets\\race\\high\\0.1.0\\5a80ba2d003e023fdce95d01c1b02f5a70d5eb2375465bee162baf9824c91474\\cache-27aafa5353895ecf.arrow\n"
     ]
    }
   ],
   "source": [
    "def make_text(row):    \n",
    "    encoded = {}\n",
    "    if USE_ANSWER:\n",
    "        s = '<answer> ' + row['answer'] + ' <context> ' + row['article']\n",
    "    else:\n",
    "        s = row['article']\n",
    "    encoded_text = tokenizer(\n",
    "        s,\n",
    "        pad_to_max_length=True, \n",
    "        max_length=SEQ_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded['input_ids'] = torch.squeeze(encoded_text['input_ids'])\n",
    "    encoded['attention_mask'] = torch.squeeze(encoded_text['attention_mask'])\n",
    "\n",
    "    encoded_question = tokenizer(\n",
    "        row['question'],\n",
    "        pad_to_max_length=True,\n",
    "        max_length=SEQ_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    encoded['input_ids_question'] = torch.squeeze(encoded_question['input_ids'])\n",
    "    return encoded\n",
    "\n",
    "dataset = dataset.map(make_text)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'input_ids_question'])\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(dataset[\"validation\"], batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NJrb9kYNz_wz"
   },
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "EPOCHS = 20\n",
    "LOG_INTERVAL = 5000\n",
    "\n",
    "config = T5Config(decoder_start_token_id=tokenizer.pad_token_id)\n",
    "model = T5ForConditionalGeneration(config).from_pretrained(PRETRAINED_MODEL)\n",
    "model.resize_token_embeddings(len(tokenizer)) # to account for new special tokens\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rcfRh2JC0CF1"
   },
   "outputs": [],
   "source": [
    "SAVED_MODEL_PATH = \"question_generator/qg_pretrained_t5_model_trained.pth\"\n",
    "\n",
    "def train(epoch, best_val_loss):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for batch_index, batch in tqdm(enumerate(train_loader)):\n",
    "        target = {\n",
    "            'input_ids': batch['input_ids_question'].to(device)\n",
    "        }\n",
    "        data = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        optimizer.zero_grad()\n",
    "        masked_labels = mask_label_padding(target['input_ids'])\n",
    "        output = model(\n",
    "            input_ids=data['input_ids'],\n",
    "            attention_mask=data['attention_mask'],\n",
    "            labels=masked_labels\n",
    "        )\n",
    "        loss = output[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch_index % LOG_INTERVAL == 0 and batch_index > 0:\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            print('| epoch {:3d} | ' \n",
    "                  '{:5d}/{:5d} batches | '\n",
    "                  'loss {:5.2f}'.format(\n",
    "                    epoch, \n",
    "                    batch_index, len(train_loader), \n",
    "                    cur_loss))\n",
    "            total_loss = 0\n",
    "\n",
    "def evaluate(eval_model, data_loader):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch in tqdm(enumerate(data_loader)):\n",
    "            target = {\n",
    "                'input_ids': batch['input_ids_question'].to(device)\n",
    "            }\n",
    "            data = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)\n",
    "            }\n",
    "            masked_labels = mask_label_padding(target['input_ids'])\n",
    "            output = eval_model(\n",
    "                input_ids=data['input_ids'],\n",
    "                attention_mask=data['attention_mask'],\n",
    "                labels=masked_labels\n",
    "            )\n",
    "            total_loss += output[0].item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def mask_label_padding(labels):\n",
    "    MASK_ID = -100\n",
    "    labels[labels==tokenizer.pad_token_id] = MASK_ID\n",
    "    return labels\n",
    "\n",
    "def load(path):\n",
    "    return torch.load(path)\n",
    "\n",
    "def print_line():\n",
    "    LINE_WIDTH = 60\n",
    "    print('-' * LINE_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9J13rDps2QIu",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5acc05a06a4d97bd96ff744cac7c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  5000/62445 batches | loss  3.75\n",
      "| epoch   1 | 10000/62445 batches | loss  3.27\n",
      "| epoch   1 | 15000/62445 batches | loss  3.07\n",
      "| epoch   1 | 20000/62445 batches | loss  2.93\n",
      "| epoch   1 | 25000/62445 batches | loss  2.84\n",
      "| epoch   1 | 30000/62445 batches | loss  2.75\n",
      "| epoch   1 | 35000/62445 batches | loss  2.66\n",
      "| epoch   1 | 40000/62445 batches | loss  2.58\n",
      "| epoch   1 | 45000/62445 batches | loss  2.53\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "print_line()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    train(epoch, best_val_loss)\n",
    "    torch.cuda.empty_cache()\n",
    "    val_loss = evaluate(model, valid_loader)\n",
    "    torch.cuda.empty_cache()\n",
    "    print_line()\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f}'.format(\n",
    "        epoch,\n",
    "        val_loss)\n",
    "    )\n",
    "    print_line()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_loss': best_val_loss,\n",
    "            'using_answer': USE_ANSWER\n",
    "        }, DIR + BEST)\n",
    "        model.save_pretrained(DIR + BEST_HF)\n",
    "        print(\"Model saved.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNkFJbYAaLQ9Q2ylMwrLSAk",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1Dv4wvUk6kl0WGFBxf2zb2W_3DDzwBLe8",
   "name": "QG_T5_training.ipynb",
   "provenance": [
    {
     "file_id": "1fjUac5wqsbl4kdfR6XRD0Zor3TfAg-fe",
     "timestamp": 1592456064028
    },
    {
     "file_id": "1y_8MTWmQnKalcdTQNLSZaoWN2KuAmUC5",
     "timestamp": 1591767629922
    },
    {
     "file_id": "1CIlJj2br71COiwuF4ZLWJPnMNgu0Z3u5",
     "timestamp": 1590466302923
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python [conda env:idl_p]",
   "language": "python",
   "name": "conda-env-idl_p-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
